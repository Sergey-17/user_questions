Создай Python-модуль для общения с LLM через Proxy API. Назови файл 'openai_module.py".

Пример подключения из документации с официального сайта:

from openai import OpenAI

client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
    base_url="https://openai.api.proxyapi.ru/v1",
)

chat_completion = client.chat.completions.create(
    model="gpt-4o", 
    messages=[
        {
            "role": "user",
            "content": "Привет!"
        }
    ]
)

2. Реализуй класс 'LLMClient:

В конструкторе загружаются, из .env с помощью dotenv: "base_url", api_key', "model'. 
И необязательные 'system_prompt', 'max_tokens'.

Должна быть возможность динамически поменять 'system_prompt', 'max_tokens'.

Сделай три метода:
chat(prompt: str) -> str - простой запрос, возвращает текст.
chat_with_system(system_prompt: str, user_prompt: str) -> str - отправка с системным промптом.
chat_json(system_prompt: str, user_prompt: str) -> dict - запрос со структурированным ответом, парсится в Python-словарь.

Добавь тестовые запуски методов в точку входа - if __name__ == "__main__":
